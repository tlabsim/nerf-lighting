{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0dffb5442d84be890217d664a91c46e420962c190ec91d07047007dfcc46efe8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import imageio\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from run_nerf_helpers import *\n",
    "from load_llff import load_llff_data\n",
    "from load_deepvoxels import load_dv_data\n",
    "from load_blender import load_blender_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(fn, chunk):\n",
    "    \"\"\"Constructs a version of 'fn' that applies to smaller batches.\"\"\"\n",
    "    if chunk is None:\n",
    "        return fn\n",
    "\n",
    "    def ret(inputs):\n",
    "        return tf.concat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n",
    "    \"\"\"Prepares inputs and applies network 'fn'.\"\"\"\n",
    "\n",
    "    inputs_flat = tf.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "\n",
    "    embedded = embed_fn(inputs_flat)\n",
    "    if viewdirs is not None:\n",
    "        # input_dirs = tf.broadcast_to(viewdirs[:, None], inputs.shape)\n",
    "        input_dirs = viewdirs\n",
    "        input_dirs_flat = tf.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
    "        embedded = tf.concat([embedded, embedded_dirs], -1)\n",
    "    \n",
    "    print(embedded.shape)\n",
    "\n",
    "    outputs_flat = batchify(fn, netchunk)(embedded)\n",
    "    outputs = tf.reshape(outputs_flat, list(\n",
    "        inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MODEL 39 39 <class 'int'> <class 'int'> False\n",
      "(None, 78) (None, 39) (None, 39)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 78)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.split (TFOpLambda)           [(None, 39), (None,  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_1 (Dense)                 (None, 256)          10240       tf.split[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_2 (Dense)                 (None, 256)          65792       layer_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_3 (Dense)                 (None, 256)          65792       layer_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_4 (Dense)                 (None, 256)          65792       layer_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_5 (Dense)                 (None, 256)          65792       layer_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 295)          0           tf.split[0][0]                   \n",
      "                                                                 layer_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_6 (Dense)                 (None, 256)          75776       tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_7 (Dense)                 (None, 256)          65792       layer_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_8 (Dense)                 (None, 256)          65792       layer_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck (Dense)              (None, 256)          65792       layer_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_1 (TFOpLambda)        (None, 295)          0           bottleneck[0][0]                 \n",
      "                                                                 tf.split[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "mu_layer_1 (Dense)              (None, 256)          75776       tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pre_rgb_layer_1 (Dense)         (None, 128)          37888       tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "mu_layer_2 (Dense)              (None, 256)          65792       mu_layer_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "rgb_out (Dense)                 (None, 3)            387         pre_rgb_layer_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "density_out (Dense)             (None, 1)            257         layer_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mu_out (Dense)                  (None, 1)            257         mu_layer_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_2 (TFOpLambda)        (None, 5)            0           rgb_out[0][0]                    \n",
      "                                                                 density_out[0][0]                \n",
      "                                                                 mu_out[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 726,917\n",
      "Trainable params: 726,917\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = init_nerf_model(D = 8, W = 256, input_ch=39, input_ch_views=39, output_ch=5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['layer_1/kernel:0', 'layer_1/bias:0', 'layer_2/kernel:0', 'layer_2/bias:0', 'layer_3/kernel:0', 'layer_3/bias:0', 'layer_4/kernel:0', 'layer_4/bias:0', 'layer_5/kernel:0', 'layer_5/bias:0', 'layer_6/kernel:0', 'layer_6/bias:0', 'layer_7/kernel:0', 'layer_7/bias:0', 'layer_8/kernel:0', 'layer_8/bias:0', 'bottleneck/kernel:0', 'bottleneck/bias:0', 'pre_rgb_layer_1/kernel:0', 'pre_rgb_layer_1/bias:0', 'rgb_out/kernel:0', 'rgb_out/bias:0', 'density_out/kernel:0', 'density_out/bias:0']\n"
     ]
    }
   ],
   "source": [
    "mu_layers = [18, 19, 22, 23, 28, 29]\n",
    "rgbd_grad_vars = []\n",
    "mu_grad_vars = []\n",
    "\n",
    "for i in range(len(model.trainable_variables)):\n",
    "    if i in mu_layers:\n",
    "        mu_grad_vars.append(model.trainable_variables[i])\n",
    "    else:\n",
    "        rgbd_grad_vars.append(model.trainable_variables[i])\n",
    "print([l.name for l in rgbd_grad_vars])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 3), dtype=float32, numpy=\n",
       "array([[4.115079  , 0.10462292, 1.2167611 ],\n",
       "       [3.6850984 , 0.98624295, 1.5599636 ],\n",
       "       [0.9351629 , 3.4922411 , 4.183166  ],\n",
       "       [0.6808875 , 3.367009  , 0.44984326],\n",
       "       [5.2027993 , 5.990637  , 4.8450036 ],\n",
       "       [3.799847  , 2.5152562 , 0.18851593],\n",
       "       [5.362183  , 4.8336473 , 5.6360407 ],\n",
       "       [1.6423205 , 0.7448357 , 0.30772966],\n",
       "       [1.2226323 , 4.662414  , 4.11778   ],\n",
       "       [0.49655363, 0.6098771 , 5.921824  ]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "pos = np.random.uniform(0, 6, (10, 3))\n",
    "pos = tf.convert_to_tensor(pos, dtype = tf.float32)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.29338959 0.06766113 0.3493246 ]\n [0.38851072 0.26263642 0.25689952]\n [0.86691224 0.73285587 0.48115803]\n [0.22167508 0.13858921 0.90797558]\n [0.93021106 0.26843926 0.23453008]\n [0.50912751 0.93004734 0.51884114]\n [0.65008238 0.14127738 0.18735449]\n [0.26497406 0.28370441 0.83673015]\n [0.6569906  0.65349762 0.92043821]\n [0.74410248 0.27032205 0.66853587]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([10, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from  sklearn.preprocessing import normalize\n",
    "view_dir = np.random.random((10, 3))\n",
    "print(view_dir)\n",
    "view_dir = normalize(view_dir, norm=\"l2\")\n",
    "view_dir = tf.convert_to_tensor(view_dir, dtype = tf.float32)\n",
    "\n",
    "view_dir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([3])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "view_dir[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "embed_fn, input_ch = get_embedder(6)\n",
    "input_ch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<dtype: 'float32'>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 39), dtype=float32, numpy=\n",
       "array([[ 4.115079  ,  0.10462292,  1.2167611 , -0.8268515 ,  0.10443216,\n",
       "         0.9379814 , -0.56242037,  0.994532  ,  0.3466856 ,  0.9300762 ,\n",
       "         0.20772226,  0.6503693 , -0.36736673,  0.97818786, -0.7596182 ,\n",
       "        -0.68335813,  0.40638277, -0.98806465, -0.7300834 ,  0.9137029 ,\n",
       "         0.1540397 ,  0.99781674,  0.7426263 , -0.30440235,  0.06604335,\n",
       "         0.6697061 , -0.95254356,  0.13179833,  0.99468267,  0.579913  ,\n",
       "        -0.99127656, -0.10298751,  0.81467843, -0.26129717, -0.20487978,\n",
       "         0.9448852 ,  0.9652584 , -0.9787871 ,  0.32740185],\n",
       "       [ 3.6850984 ,  0.98624295,  1.5599636 , -0.51713973,  0.8339586 ,\n",
       "         0.99994135, -0.855901  ,  0.55182695,  0.01083253,  0.88524085,\n",
       "         0.92040175,  0.02166379,  0.46513298, -0.390974  , -0.99976534,\n",
       "         0.8235094 , -0.71970624, -0.04331741, -0.56730264, -0.6942787 ,\n",
       "         0.99906135, -0.9343581 ,  0.9993534 , -0.08655351, -0.35633543,\n",
       "        -0.03595421,  0.99624723,  0.6658898 , -0.07186194, -0.17245738,\n",
       "        -0.7460501 , -0.9974146 ,  0.985017  , -0.9935743 ,  0.1433523 ,\n",
       "        -0.3397469 ,  0.11318155,  0.9896717 ,  0.9405169 ],\n",
       "       [ 0.9351629 ,  3.4922411 ,  4.183166  ,  0.80469584, -0.3435069 ,\n",
       "        -0.86319965,  0.59368736, -0.93915015, -0.5048628 ,  0.95547545,\n",
       "         0.64520913,  0.87159467, -0.29507065,  0.764006  , -0.49022722,\n",
       "        -0.56386554,  0.9858873 , -0.8545589 , -0.82586664,  0.16741036,\n",
       "        -0.5193546 ,  0.9313555 ,  0.3300955 ,  0.8876381 ,  0.3641113 ,\n",
       "        -0.94394755, -0.4605417 ,  0.6782341 , -0.6231857 , -0.8175887 ,\n",
       "        -0.73484594,  0.782074  , -0.5758027 , -0.9967951 , -0.9747545 ,\n",
       "         0.9415395 ,  0.07999708,  0.2232793 , -0.33690247],\n",
       "       [ 0.6808875 ,  3.367009  ,  0.44984326,  0.6294829 , -0.22351213,\n",
       "         0.43482438,  0.7770144 , -0.97470117,  0.90051526,  0.97823447,\n",
       "         0.43571505,  0.783132  ,  0.20750259,  0.9000847 ,  0.6218555 ,\n",
       "         0.40597236,  0.7843609 ,  0.9739899 , -0.91388535,  0.6203048 ,\n",
       "        -0.22659148, -0.7420244 ,  0.9730856 , -0.4413956 ,  0.6703729 ,\n",
       "        -0.23044391, -0.89731264, -0.9948661 , -0.4484833 ,  0.7921397 ,\n",
       "        -0.10120042, -0.8937912 ,  0.6103398 ,  0.20136173,  0.8017009 ,\n",
       "         0.9669488 , -0.9795169 ,  0.59772545, -0.2549706 ],\n",
       "       [ 5.2027993 ,  5.990637  ,  4.8450036 , -0.8821397 , -0.28839335,\n",
       "        -0.9912196 ,  0.4709879 ,  0.957512  ,  0.13222626, -0.83095425,\n",
       "        -0.55228025, -0.2621305 , -0.5563408 ,  0.8336586 , -0.96503246,\n",
       "         0.9245875 , -0.92082626,  0.5059289 , -0.38096985,  0.3899731 ,\n",
       "         0.8625752 , -0.70447993, -0.718195  ,  0.8728034 , -0.70972395,\n",
       "        -0.69584197,  0.48807195,  0.9999725 ,  0.99950033,  0.8519817 ,\n",
       "         0.00741613, -0.03160801, -0.5235716 ,  0.01483185, -0.06318443,\n",
       "        -0.8921468 , -0.99989   , -0.9980019 , -0.4517456 ],\n",
       "       [ 3.799847  ,  2.5152562 ,  0.18851593, -0.6117368 ,  0.5861806 ,\n",
       "         0.18740132, -0.7910614 , -0.8101804 ,  0.9822834 ,  0.9678427 ,\n",
       "        -0.9498241 ,  0.36816242,  0.25155625,  0.3127846 ,  0.92976147,\n",
       "         0.48693374, -0.5941807 ,  0.68460655, -0.8734389 , -0.8043316 ,\n",
       "         0.7289129 , -0.8506137 ,  0.95583665,  0.9980369 ,  0.5257911 ,\n",
       "         0.2938985 ,  0.06262786, -0.89449024,  0.5618379 ,  0.12500983,\n",
       "        -0.44708744, -0.8272474 , -0.9921555 ,  0.79983073, -0.9295578 ,\n",
       "        -0.24805841, -0.6002257 ,  0.36867636,  0.9687451 ],\n",
       "       [ 5.362183  ,  4.8336473 ,  5.6360407 , -0.7962084 , -0.99265724,\n",
       "        -0.6029108 ,  0.6050225 ,  0.12096134,  0.7978086 , -0.9634479 ,\n",
       "        -0.2401463 , -0.96201485, -0.26789558, -0.9707367 ,  0.27299708,\n",
       "         0.5162069 ,  0.46623763, -0.5252545 , -0.8564639 ,  0.8846595 ,\n",
       "        -0.8509452 , -0.88422513,  0.8249231 ,  0.8939256 ,  0.46706092,\n",
       "         0.565245  ,  0.4482154 , -0.825974  ,  0.93256724,  0.8013424 ,\n",
       "        -0.56370825, -0.3609963 , -0.5982059 ,  0.93121666, -0.67330664,\n",
       "        -0.9587356 , -0.3644661 , -0.7393634 , -0.28429937],\n",
       "       [ 1.6423205 ,  0.7448357 ,  0.30772966,  0.99744326,  0.677851  ,\n",
       "         0.30289572, -0.07146322,  0.73519933,  0.95302373, -0.14256102,\n",
       "         0.9967112 ,  0.5773336 , -0.989786  ,  0.08103602,  0.81650835,\n",
       "         0.28220978,  0.16153902,  0.94279546,  0.95935273, -0.98686635,\n",
       "         0.3333718 ,  0.54147744, -0.31883484,  0.6286028 ,  0.8407153 ,\n",
       "         0.9478103 , -0.77772653,  0.9104567 , -0.6043899 , -0.9777621 ,\n",
       "         0.41360432,  0.79668874,  0.2097171 ,  0.7531377 , -0.96302116,\n",
       "        -0.41010684, -0.6578629 ,  0.26942575, -0.9120375 ],\n",
       "       [ 1.2226323 ,  4.662414  ,  4.11778   ,  0.94000065, -0.9987515 ,\n",
       "        -0.8283677 ,  0.3411726 , -0.04995411, -0.5601847 ,  0.6414049 ,\n",
       "         0.09978348,  0.9280779 , -0.76720256, -0.9950092 , -0.37238616,\n",
       "        -0.98417497, -0.19857097, -0.6912067 ,  0.1771995 ,  0.9800865 ,\n",
       "        -0.72265714, -0.34879062, -0.38923344,  0.99901086, -0.93720067,\n",
       "         0.9211392 ,  0.04446653,  0.6537736 , -0.7170763 ,  0.08884509,\n",
       "         0.75669026,  0.69699466, -0.9960455 ,  0.98940814, -0.9995967 ,\n",
       "        -0.1769875 ,  0.14516024, -0.02839686,  0.9842131 ],\n",
       "       [ 0.49655363,  0.6098771 ,  5.921824  ,  0.47639823,  0.5727667 ,\n",
       "        -0.35354796,  0.8792296 ,  0.8197184 ,  0.9354164 ,  0.8377269 ,\n",
       "         0.93901485, -0.66142917,  0.5460895 ,  0.34387657,  0.7500077 ,\n",
       "         0.9149476 ,  0.6458104 , -0.9921538 , -0.4035726 , -0.7634978 ,\n",
       "         0.12502299, -0.7384956 , -0.98614967, -0.2480841 , -0.67425835,\n",
       "         0.1658579 , -0.9687385 ,  0.9958736 , -0.32712144,  0.48065725,\n",
       "        -0.0907514 , -0.9449823 ,  0.8769086 , -0.18075386,  0.6182479 ,\n",
       "         0.8429849 , -0.9835284 ,  0.78598315,  0.5379372 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "inputs = pos\n",
    "inputs_flat = tf.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "print(inputs_flat.dtype)\n",
    "embedded = embed_fn(inputs_flat)\n",
    "embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10, 78)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 3), dtype=float32, numpy=\n",
       "array([[ 0.41990867,  0.02888134,  0.4073558 ],\n",
       "       [ 0.2096487 ,  0.6944843 ,  0.12844644],\n",
       "       [ 0.49046126,  0.47913814,  0.15657158],\n",
       "       [ 0.4056679 ,  0.24762534,  0.31827408],\n",
       "       [ 0.57932806, -0.10852477,  0.266086  ],\n",
       "       [-0.06878987,  0.5967194 , -0.03214411],\n",
       "       [ 0.47685555,  0.05749227,  0.13453513],\n",
       "       [ 0.00341313,  0.10272586,  0.2731193 ],\n",
       "       [ 0.43433943, -0.00269427,  0.13577439],\n",
       "       [ 0.32900867,  0.21789443,  0.26277596]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "out = run_network(pos, view_dir, model, embed_fn, embed_fn, 2)\n",
    "rgb, density, mu = out[:, :3], out[:, 3], out[:, 4]\n",
    "rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def render_rays(ray_batch,\n",
    "                network_fn,\n",
    "                network_query_fn,\n",
    "                N_samples,\n",
    "                retraw=False,\n",
    "                lindisp=False,\n",
    "                perturb=0.,\n",
    "                N_importance=0,\n",
    "                network_fine=None,\n",
    "                white_bkgd=False,\n",
    "                raw_noise_std=0.,\n",
    "                verbose=False):\n",
    "    \"\"\"Volumetric rendering.\n",
    "\n",
    "    Args:\n",
    "      ray_batch: array of shape [batch_size, ...]. All information necessary\n",
    "        for sampling along a ray, including: ray origin, ray direction, min\n",
    "        dist, max dist, and unit-magnitude viewing direction.\n",
    "      network_fn: function. Model for predicting RGB and density at each point\n",
    "        in space.\n",
    "      network_query_fn: function used for passing queries to network_fn.\n",
    "      N_samples: int. Number of different times to sample along each ray.\n",
    "      retraw: bool. If True, include model's raw, unprocessed predictions.\n",
    "      lindisp: bool. If True, sample linearly in inverse depth rather than in depth.\n",
    "      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified\n",
    "        random points in time.\n",
    "      N_importance: int. Number of additional times to sample along each ray.\n",
    "        These samples are only passed to network_fine.\n",
    "      network_fine: \"fine\" network with same spec as network_fn.\n",
    "      white_bkgd: bool. If True, assume a white background.\n",
    "      raw_noise_std: ...\n",
    "      verbose: bool. If True, print more debugging info.\n",
    "\n",
    "    Returns:\n",
    "      rgb_map: [num_rays, 3]. Estimated RGB color of a ray. Comes from fine model.\n",
    "      disp_map: [num_rays]. Disparity map. 1 / depth.\n",
    "      acc_map: [num_rays]. Accumulated opacity along each ray. Comes from fine model.\n",
    "      raw: [num_rays, num_samples, 4]. Raw predictions from model.\n",
    "      rgb0: See rgb_map. Output for coarse model.\n",
    "      disp0: See disp_map. Output for coarse model.\n",
    "      acc0: See acc_map. Output for coarse model.\n",
    "      z_std: [num_rays]. Standard deviation of distances along ray for each\n",
    "        sample.\n",
    "    \"\"\"\n",
    "\n",
    "    def raw2outputs(raw, z_vals, rays_d):\n",
    "        \"\"\"Transforms model's predictions to semantically meaningful values.\n",
    "\n",
    "        Args:\n",
    "          raw: [num_rays, num_samples along ray, 4]. Prediction from model.\n",
    "          z_vals: [num_rays, num_samples along ray]. Integration time.\n",
    "          rays_d: [num_rays, 3]. Direction of each ray.\n",
    "\n",
    "        Returns:\n",
    "          rgb_map: [num_rays, 3]. Estimated RGB color of a ray.\n",
    "          disp_map: [num_rays]. Disparity map. Inverse of depth map.\n",
    "          acc_map: [num_rays]. Sum of weights along each ray.\n",
    "          weights: [num_rays, num_samples]. Weights assigned to each sampled color.\n",
    "          depth_map: [num_rays]. Estimated distance to object.\n",
    "        \"\"\"\n",
    "        # Function for computing density from model prediction. This value is\n",
    "        # strictly between [0, 1].\n",
    "        def raw2alpha(raw, dists, act_fn=tf.nn.relu): return 1.0 - \\\n",
    "            tf.exp(-act_fn(raw) * dists)\n",
    "\n",
    "        # Compute 'distance' (in time) between each integration time along a ray.\n",
    "        dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "\n",
    "        # The 'distance' from the last integration time is infinity.\n",
    "        dists = tf.concat(\n",
    "            [dists, tf.broadcast_to([1e10], dists[..., :1].shape)],\n",
    "            axis=-1)  # [N_rays, N_samples]\n",
    "\n",
    "        # Multiply each distance by the norm of its corresponding direction ray\n",
    "        # to convert to real world distance (accounts for non-unit directions).\n",
    "        dists = dists * tf.linalg.norm(rays_d[..., None, :], axis=-1)\n",
    "\n",
    "        # Extract RGB of each sample position along each ray.\n",
    "        rgb = tf.math.sigmoid(raw[..., :3])  # [N_rays, N_samples, 3]\n",
    "\n",
    "        # Add noise to model's predictions for density. Can be used to \n",
    "        # regularize network during training (prevents floater artifacts).\n",
    "        noise = 0.\n",
    "        if raw_noise_std > 0.:\n",
    "            noise = tf.random.normal(raw[..., 3].shape) * raw_noise_std\n",
    "\n",
    "        # Predict density of each sample along each ray. Higher values imply\n",
    "        # higher likelihood of being absorbed at this point.\n",
    "        alpha = raw2alpha(raw[..., 3] + noise, dists)  # [N_rays, N_samples]\n",
    "\n",
    "        # Compute weight for RGB of each sample along each ray.  A cumprod() is\n",
    "        # used to express the idea of the ray not having reflected up to this\n",
    "        # sample yet.\n",
    "        # [N_rays, N_samples]\n",
    "\n",
    "        mu_expected = tf.math.cumprod(1.-alpha + 1e-10, axis=-1, exclusive=True)\n",
    "        weights = alpha * mu_expected                 \n",
    "\n",
    "        # Computed weighted color of each sample along each ray.\n",
    "        rgb_map = tf.reduce_sum(weights[..., None] * rgb, axis=-2)  # [N_rays, 3]\n",
    "\n",
    "        # Estimated depth map is expected distance.\n",
    "        depth_map = tf.reduce_sum(weights * z_vals, axis=-1)\n",
    "\n",
    "        # Disparity map is inverse depth.\n",
    "        disp_map = 1./tf.maximum(1e-10, depth_map /\n",
    "                                 tf.reduce_sum(weights, axis=-1))\n",
    "\n",
    "        # Sum of weights along each ray. This value is in [0, 1] up to numerical error.\n",
    "        acc_map = tf.reduce_sum(weights, -1)\n",
    "\n",
    "        mu_map = tf.nn.relu(raw[..., 4])   \n",
    "\n",
    "        # To composite onto a white background, use the accumulated alpha map.\n",
    "        if white_bkgd:\n",
    "            rgb_map = rgb_map + (1.-acc_map[..., None])\n",
    "\n",
    "        return rgb_map, disp_map, acc_map, weights, depth_map, mu_map, mu_expected\n",
    "\n",
    "    ###############################\n",
    "    # batch size\n",
    "    N_rays = ray_batch.shape[0]\n",
    "\n",
    "    # Extract ray origin, direction.\n",
    "    rays_o, rays_d = ray_batch[:, 0:3], ray_batch[:, 3:6]  # [N_rays, 3] each\n",
    "\n",
    "    # Extract unit-normalized viewing direction.\n",
    "    viewdirs = ray_batch[:, -3:] if ray_batch.shape[-1] > 8 else None\n",
    "\n",
    "    # Extract lower, upper bound for ray distance.\n",
    "    bounds = tf.reshape(ray_batch[..., 6:8], [-1, 1, 2])\n",
    "    near, far = bounds[..., 0], bounds[..., 1]  # [-1,1]\n",
    "\n",
    "    # Decide where to sample along each ray. Under the logic, all rays will be sampled at\n",
    "    # the same times.\n",
    "    t_vals = tf.linspace(0., 1., N_samples)\n",
    "    if not lindisp:\n",
    "        # Space integration times linearly between 'near' and 'far'. Same\n",
    "        # integration points will be used for all rays.\n",
    "        z_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "    else:\n",
    "        # Sample linearly in inverse depth (disparity).\n",
    "        z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "    z_vals = tf.broadcast_to(z_vals, [N_rays, N_samples])\n",
    "\n",
    "    # Perturb sampling time along each ray.\n",
    "    if perturb > 0.:\n",
    "        # get intervals between samples\n",
    "        mids = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "        upper = tf.concat([mids, z_vals[..., -1:]], -1)\n",
    "        lower = tf.concat([z_vals[..., :1], mids], -1)\n",
    "        # stratified samples in those intervals\n",
    "        t_rand = tf.random.uniform(z_vals.shape)\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "    # Points in space to evaluate model at.\n",
    "    pts = rays_o[..., None, :] + rays_d[..., None, :] * \\\n",
    "        z_vals[..., :, None]  # [N_rays, N_samples, 3]\n",
    "\n",
    "    # Evaluate model at each point.\n",
    "    raw = network_query_fn(pts, viewdirs, network_fn)  # [N_rays, N_samples, 4]\n",
    "    rgb_map, disp_map, acc_map, weights, depth_map, mu_map, mu_exp = \\\n",
    "        raw2outputs(raw, z_vals, rays_d)\n",
    "\n",
    "    if N_importance > 0:\n",
    "        rgb_map_0, disp_map_0, acc_map_0, mu_map_0, mu_exp_0 = \\\n",
    "            rgb_map, disp_map, acc_map, mu_map, mu_exp\n",
    "\n",
    "        # Obtain additional integration times to evaluate based on the weights\n",
    "        # assigned to colors in the coarse model.\n",
    "        z_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "        z_samples = sample_pdf(\n",
    "            z_vals_mid, weights[..., 1:-1], N_importance, det=(perturb == 0.))\n",
    "        z_samples = tf.stop_gradient(z_samples)\n",
    "\n",
    "        # Obtain all points to evaluate color, density at.\n",
    "        z_vals = tf.sort(tf.concat([z_vals, z_samples], -1), -1)\n",
    "        pts = rays_o[..., None, :] + rays_d[..., None, :] * \\\n",
    "            z_vals[..., :, None]  # [N_rays, N_samples + N_importance, 3]\n",
    "\n",
    "        # Make predictions with network_fine.\n",
    "        run_fn = network_fn if network_fine is None else network_fine\n",
    "        raw = network_query_fn(pts, viewdirs, run_fn)\n",
    "        rgb_map, disp_map, acc_map, weights, depth_map, mu_map, mu_exp = \\\n",
    "            raw2outputs(raw, z_vals, rays_d)\n",
    "\n",
    "    ret = {'rgb_map': rgb_map, 'disp_map': disp_map, 'acc_map': acc_map, \\\n",
    "        'mu_map': mu_map, 'mu_exp': mu_exp}\n",
    "    if retraw:\n",
    "        ret['raw'] = raw\n",
    "    if N_importance > 0:\n",
    "        ret['rgb0'] = rgb_map_0\n",
    "        ret['disp0'] = disp_map_0\n",
    "        ret['acc0'] = acc_map_0        \n",
    "        ret['mu_map0'] = mu_map_0\n",
    "        ret['mu_exp0'] = mu_exp_0\n",
    "        ret['z_std'] = tf.math.reduce_std(z_samples, -1)  # [N_rays]\n",
    "\n",
    "    for k in ret:\n",
    "        tf.debugging.check_numerics(ret[k], 'output {}'.format(k))\n",
    "\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_rays(rays_flat, chunk=1024*32, **kwargs):\n",
    "    \"\"\"Render rays in smaller minibatches to avoid OOM.\"\"\"\n",
    "    all_ret = {}\n",
    "    for i in range(0, rays_flat.shape[0], chunk):\n",
    "        ret = render_rays(rays_flat[i:i+chunk], **kwargs)\n",
    "        for k in ret:\n",
    "            if k not in all_ret:\n",
    "                all_ret[k] = []\n",
    "            all_ret[k].append(ret[k])\n",
    "\n",
    "    all_ret = {k: tf.concat(all_ret[k], 0) for k in all_ret}\n",
    "    return all_ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(H, W, focal,\n",
    "           chunk=1024*32, rays=None, c2w=None, ndc=True,\n",
    "           near=0., far=1.,\n",
    "           use_viewdirs=False, c2w_staticcam=None,\n",
    "           **kwargs):\n",
    "    \"\"\"Render rays\n",
    "\n",
    "    Args:\n",
    "      H: int. Height of image in pixels.\n",
    "      W: int. Width of image in pixels.\n",
    "      focal: float. Focal length of pinhole camera.\n",
    "      chunk: int. Maximum number of rays to process simultaneously. Used to\n",
    "        control maximum memory usage. Does not affect final results.\n",
    "      rays: array of shape [2, batch_size, 3]. Ray origin and direction for\n",
    "        each example in batch.\n",
    "      c2w: array of shape [3, 4]. Camera-to-world transformation matrix.\n",
    "      ndc: bool. If True, represent ray origin, direction in NDC coordinates.\n",
    "      near: float or array of shape [batch_size]. Nearest distance for a ray.\n",
    "      far: float or array of shape [batch_size]. Farthest distance for a ray.\n",
    "      use_viewdirs: bool. If True, use viewing direction of a point in space in model.\n",
    "      c2w_staticcam: array of shape [3, 4]. If not None, use this transformation matrix for \n",
    "       camera while using other c2w argument for viewing directions.\n",
    "\n",
    "    Returns:\n",
    "      rgb_map: [batch_size, 3]. Predicted RGB values for rays.\n",
    "      disp_map: [batch_size]. Disparity map. Inverse of depth.\n",
    "      acc_map: [batch_size]. Accumulated opacity (alpha) along a ray.\n",
    "      extras: dict with everything returned by render_rays().\n",
    "    \"\"\"\n",
    "\n",
    "    if c2w is not None:\n",
    "        # special case to render full image\n",
    "        rays_o, rays_d = get_rays(H, W, focal, c2w)\n",
    "    else:\n",
    "        # use provided ray batch\n",
    "        rays_o, rays_d = rays\n",
    "\n",
    "    if use_viewdirs:\n",
    "        # provide ray directions as input\n",
    "        viewdirs = rays_d\n",
    "        if c2w_staticcam is not None:\n",
    "            # special case to visualize effect of viewdirs\n",
    "            rays_o, rays_d = get_rays(H, W, focal, c2w_staticcam)\n",
    "\n",
    "        # Make all directions unit magnitude.\n",
    "        # shape: [batch_size, 3]\n",
    "        viewdirs = viewdirs / tf.linalg.norm(viewdirs, axis=-1, keepdims=True)\n",
    "        viewdirs = tf.cast(tf.reshape(viewdirs, [-1, 3]), dtype=tf.float32)\n",
    "\n",
    "    sh = rays_d.shape  # [..., 3]\n",
    "    if ndc:\n",
    "        # for forward facing scenes\n",
    "        rays_o, rays_d = ndc_rays(\n",
    "            H, W, focal, tf.cast(1., tf.float32), rays_o, rays_d)\n",
    "\n",
    "    # Create ray batch\n",
    "    rays_o = tf.cast(tf.reshape(rays_o, [-1, 3]), dtype=tf.float32)\n",
    "    rays_d = tf.cast(tf.reshape(rays_d, [-1, 3]), dtype=tf.float32)\n",
    "    near, far = near * \\\n",
    "        tf.ones_like(rays_d[..., :1]), far * tf.ones_like(rays_d[..., :1])\n",
    "\n",
    "    # (ray origin, ray direction, min dist, max dist) for each ray\n",
    "    rays = tf.concat([rays_o, rays_d, near, far], axis=-1)\n",
    "    if use_viewdirs:\n",
    "        # (ray origin, ray direction, min dist, max dist, normalized viewing direction)\n",
    "        rays = tf.concat([rays, viewdirs], axis=-1)\n",
    "\n",
    "    # Render and reshape\n",
    "    all_ret = batchify_rays(rays, chunk, **kwargs)\n",
    "    for k in all_ret:\n",
    "        k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:])\n",
    "        all_ret[k] = tf.reshape(all_ret[k], k_sh)\n",
    "\n",
    "    k_extract = ['rgb_map', 'disp_map', 'acc_map', 'mu_map', 'mu_exp']\n",
    "    ret_list = [all_ret[k] for k in k_extract]\n",
    "    ret_dict = {k: all_ret[k] for k in all_ret if k not in k_extract}\n",
    "    return ret_list + [ret_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_path(render_poses, hwf, chunk, render_kwargs, gt_imgs=None, savedir=None, render_factor=0):\n",
    "\n",
    "    H, W, focal = hwf\n",
    "\n",
    "    if render_factor != 0:\n",
    "        # Render downsampled for speed\n",
    "        H = H//render_factor\n",
    "        W = W//render_factor\n",
    "        focal = focal/render_factor\n",
    "\n",
    "    rgbs = []\n",
    "    disps = []\n",
    "\n",
    "    t = time.time()\n",
    "    for i, c2w in enumerate(render_poses):\n",
    "        print(i, time.time() - t)\n",
    "        t = time.time()\n",
    "        rgb, disp, acc, _, _, _ = render(\n",
    "            H, W, focal, chunk=chunk, c2w=c2w[:3, :4], **render_kwargs)\n",
    "        rgbs.append(rgb.numpy())\n",
    "        disps.append(disp.numpy())\n",
    "        if i == 0:\n",
    "            print(rgb.shape, disp.shape)\n",
    "\n",
    "        if gt_imgs is not None and render_factor == 0:\n",
    "            p = -10. * np.log10(np.mean(np.square(rgb - gt_imgs[i])))\n",
    "            print(p)\n",
    "\n",
    "        if savedir is not None:\n",
    "            rgb8 = to8b(rgbs[-1])\n",
    "            filename = os.path.join(savedir, '{:03d}.png'.format(i))\n",
    "            imageio.imwrite(filename, rgb8)\n",
    "\n",
    "    rgbs = np.stack(rgbs, 0)\n",
    "    disps = np.stack(disps, 0)\n",
    "\n",
    "    return rgbs, disps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_trainable_vars(model):\n",
    "    mu_layers = [18, 19, 22, 23, 28, 29]\n",
    "    rgbd_grad_vars = []\n",
    "    mu_grad_vars = []\n",
    "\n",
    "    for i in range(len(model.trainable_variables)):\n",
    "        if i in mu_layers:\n",
    "            mu_grad_vars.append(model.trainable_variables[i])\n",
    "        else:\n",
    "            rgbd_grad_vars.append(model.trainable_variables[i])\n",
    "    return rgbd_grad_vars, mu_grad_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nerf(args):\n",
    "    \"\"\"Instantiate NeRF's MLP model.\"\"\"\n",
    "\n",
    "    embed_fn, input_ch = get_embedder(args.multires, args.i_embed)\n",
    "\n",
    "    input_ch_views = 0\n",
    "    embeddirs_fn = None\n",
    "    if args.use_viewdirs:\n",
    "        embeddirs_fn, input_ch_views = get_embedder(\n",
    "            args.multires_views, args.i_embed)\n",
    "    output_ch = 4\n",
    "    skips = [4]\n",
    "    model = init_nerf_model(\n",
    "        D=args.netdepth, W=args.netwidth,\n",
    "        input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "        input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs)\n",
    "    rgbd_grad_vars, mu_grad_vars = split_trainable_vars(model)\n",
    "    models = {'model': model}\n",
    "\n",
    "    model_fine = None\n",
    "    if args.N_importance > 0:\n",
    "        model_fine = init_nerf_model(\n",
    "            D=args.netdepth_fine, W=args.netwidth_fine,\n",
    "            input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "            input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs)\n",
    "        rgbd_grad_vars_fine, mu_grad_vars_fine = split_trainable_vars(model_fine)\n",
    "        rgbd_grad_vars += rgbd_grad_vars_fine\n",
    "        mu_grad_vars += mu_grad_vars\n",
    "        models['model_fine'] = model_fine\n",
    "    grad_vars = [rgbd_grad_vars, mu_grad_vars]\n",
    "\n",
    "    def network_query_fn(inputs, viewdirs, network_fn): return run_network(\n",
    "        inputs, viewdirs, network_fn,\n",
    "        embed_fn=embed_fn,\n",
    "        embeddirs_fn=embeddirs_fn,\n",
    "        netchunk=args.netchunk)\n",
    "\n",
    "    render_kwargs_train = {\n",
    "        'network_query_fn': network_query_fn,\n",
    "        'perturb': args.perturb,\n",
    "        'N_importance': args.N_importance,\n",
    "        'network_fine': model_fine,\n",
    "        'N_samples': args.N_samples,\n",
    "        'network_fn': model,\n",
    "        'use_viewdirs': args.use_viewdirs,\n",
    "        'white_bkgd': args.white_bkgd,\n",
    "        'raw_noise_std': args.raw_noise_std,\n",
    "    }\n",
    "\n",
    "    # NDC only good for LLFF-style forward facing data\n",
    "    if args.dataset_type != 'llff' or args.no_ndc:\n",
    "        print('Not ndc!')\n",
    "        render_kwargs_train['ndc'] = False\n",
    "        render_kwargs_train['lindisp'] = args.lindisp\n",
    "\n",
    "    render_kwargs_test = {\n",
    "        k: render_kwargs_train[k] for k in render_kwargs_train}\n",
    "    render_kwargs_test['perturb'] = False\n",
    "    render_kwargs_test['raw_noise_std'] = 0.\n",
    "\n",
    "    start = 0\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "\n",
    "    if args.ft_path is not None and args.ft_path != 'None':\n",
    "        ckpts = [args.ft_path]\n",
    "    else:\n",
    "        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if\n",
    "                 ('model_' in f and 'fine' not in f and 'optimizer' not in f)]\n",
    "    print('Found ckpts', ckpts)\n",
    "    if len(ckpts) > 0 and not args.no_reload:\n",
    "        ft_weights = ckpts[-1]\n",
    "        print('Reloading from', ft_weights)\n",
    "        model.set_weights(np.load(ft_weights, allow_pickle=True))\n",
    "        start = int(ft_weights[-10:-4]) + 1\n",
    "        print('Resetting step to', start)\n",
    "\n",
    "        if model_fine is not None:\n",
    "            ft_weights_fine = '{}_fine_{}'.format(\n",
    "                ft_weights[:-11], ft_weights[-10:])\n",
    "            print('Reloading fine from', ft_weights_fine)\n",
    "            model_fine.set_weights(np.load(ft_weights_fine, allow_pickle=True))\n",
    "\n",
    "    return render_kwargs_train, render_kwargs_test, start, grad_vars, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_parser():\n",
    "\n",
    "    import configargparse\n",
    "    parser = configargparse.ArgumentParser()\n",
    "    parser.add_argument('--config', is_config_file=True,\n",
    "                        default = \"config_rubiks_cropped.txt\", help='config file path')\n",
    "    parser.add_argument(\"--expname\", type=str, help='experiment name')\n",
    "    parser.add_argument(\"--basedir\", type=str, default='./data/rubiks/cropped/logs/',\n",
    "                        help='where to store ckpts and logs')\n",
    "    parser.add_argument(\"--datadir\", type=str,\n",
    "                        default='./data/rubiks/cropped', help='input data directory')\n",
    "\n",
    "    # training options\n",
    "    parser.add_argument(\"--netdepth\", type=int, default=8,\n",
    "                        help='layers in network')\n",
    "    parser.add_argument(\"--netwidth\", type=int, default=256,\n",
    "                        help='channels per layer')\n",
    "    parser.add_argument(\"--netdepth_fine\", type=int,\n",
    "                        default=8, help='layers in fine network')\n",
    "    parser.add_argument(\"--netwidth_fine\", type=int, default=256,\n",
    "                        help='channels per layer in fine network')\n",
    "    parser.add_argument(\"--N_rand\", type=int, default=32*32*4,\n",
    "                        help='batch size (number of random rays per gradient step)')\n",
    "    parser.add_argument(\"--lrate\", type=float,\n",
    "                        default=5e-4, help='learning rate')\n",
    "    parser.add_argument(\"--lrate_decay\", type=int, default=250,\n",
    "                        help='exponential learning rate decay (in 1000s)')\n",
    "    parser.add_argument(\"--chunk\", type=int, default=1024*32,\n",
    "                        help='number of rays processed in parallel, decrease if running out of memory')\n",
    "    parser.add_argument(\"--netchunk\", type=int, default=1024*64,\n",
    "                        help='number of pts sent through network in parallel, decrease if running out of memory')\n",
    "    parser.add_argument(\"--no_batching\", action='store_true',\n",
    "                        help='only take random rays from 1 image at a time')\n",
    "    parser.add_argument(\"--no_reload\", action='store_true',\n",
    "                        help='do not reload weights from saved ckpt')\n",
    "    parser.add_argument(\"--ft_path\", type=str, default=None,\n",
    "                        help='specific weights npy file to reload for coarse network')\n",
    "    parser.add_argument(\"--random_seed\", type=int, default=None,\n",
    "                        help='fix random seed for repeatability')\n",
    "    \n",
    "    # pre-crop options\n",
    "    parser.add_argument(\"--precrop_iters\", type=int, default=0,\n",
    "                        help='number of steps to train on central crops')\n",
    "    parser.add_argument(\"--precrop_frac\", type=float,\n",
    "                        default=.5, help='fraction of img taken for central crops')    \n",
    "\n",
    "    # rendering options\n",
    "    parser.add_argument(\"--N_samples\", type=int, default=64,\n",
    "                        help='number of coarse samples per ray')\n",
    "    parser.add_argument(\"--N_importance\", type=int, default=0,\n",
    "                        help='number of additional fine samples per ray')\n",
    "    parser.add_argument(\"--perturb\", type=float, default=1.,\n",
    "                        help='set to 0. for no jitter, 1. for jitter')\n",
    "    parser.add_argument(\"--use_viewdirs\", action='store_true',\n",
    "                        help='use full 5D input instead of 3D')\n",
    "    parser.add_argument(\"--i_embed\", type=int, default=0,\n",
    "                        help='set 0 for default positional encoding, -1 for none')\n",
    "    parser.add_argument(\"--multires\", type=int, default=10,\n",
    "                        help='log2 of max freq for positional encoding (3D location)')\n",
    "    parser.add_argument(\"--multires_views\", type=int, default=4,\n",
    "                        help='log2 of max freq for positional encoding (2D direction)')\n",
    "    parser.add_argument(\"--raw_noise_std\", type=float, default=0.,\n",
    "                        help='std dev of noise added to regularize sigma_a output, 1e0 recommended')\n",
    "\n",
    "    parser.add_argument(\"--render_only\", action='store_true',\n",
    "                        help='do not optimize, reload weights and render out render_poses path')\n",
    "    parser.add_argument(\"--render_test\", action='store_true',\n",
    "                        help='render the test set instead of render_poses path')\n",
    "    parser.add_argument(\"--render_factor\", type=int, default=0,\n",
    "                        help='downsampling factor to speed up rendering, set 4 or 8 for fast preview')\n",
    "\n",
    "    # dataset options\n",
    "    parser.add_argument(\"--dataset_type\", type=str, default='llff',\n",
    "                        help='options: llff / blender / deepvoxels')\n",
    "    parser.add_argument(\"--testskip\", type=int, default=8,\n",
    "                        help='will load 1/N images from test/val sets, useful for large datasets like deepvoxels')\n",
    "\n",
    "    # deepvoxels flags\n",
    "    parser.add_argument(\"--shape\", type=str, default='greek',\n",
    "                        help='options : armchair / cube / greek / vase')\n",
    "\n",
    "    # blender flags\n",
    "    parser.add_argument(\"--white_bkgd\", action='store_true',\n",
    "                        help='set to render synthetic data on a white bkgd (always use for dvoxels)')\n",
    "    parser.add_argument(\"--half_res\", action='store_true',\n",
    "                        help='load blender synthetic data at 400x400 instead of 800x800')\n",
    "\n",
    "    # llff flags\n",
    "    parser.add_argument(\"--factor\", type=int, default=8,\n",
    "                        help='downsample factor for LLFF images')\n",
    "    parser.add_argument(\"--no_ndc\", action='store_true',\n",
    "                        help='do not use normalized device coordinates (set for non-forward facing scenes)')\n",
    "    parser.add_argument(\"--lindisp\", action='store_true',\n",
    "                        help='sampling linearly in disparity rather than depth')\n",
    "    parser.add_argument(\"--spherify\", action='store_true',\n",
    "                        help='set for spherical 360 scenes')\n",
    "    parser.add_argument(\"--llffhold\", type=int, default=8,\n",
    "                        help='will take every 1/N images as LLFF test set, paper uses 8')\n",
    "\n",
    "    # logging/saving options\n",
    "    parser.add_argument(\"--i_print\",   type=int, default=100,\n",
    "                        help='frequency of console printout and metric loggin')\n",
    "    parser.add_argument(\"--i_img\",     type=int, default=500,\n",
    "                        help='frequency of tensorboard image logging')\n",
    "    parser.add_argument(\"--i_weights\", type=int, default=10000,\n",
    "                        help='frequency of weight ckpt saving')\n",
    "    parser.add_argument(\"--i_testset\", type=int, default=50000,\n",
    "                        help='frequency of testset saving')\n",
    "    parser.add_argument(\"--i_video\",   type=int, default=50000,\n",
    "                        help='frequency of render_poses video saving')\n",
    "\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    parser = config_parser()\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.random_seed is not None:\n",
    "        print('Fixing random seed', args.random_seed)\n",
    "        np.random.seed(args.random_seed)\n",
    "        tf.compat.v1.set_random_seed(args.random_seed)\n",
    "\n",
    "    # Load data\n",
    "\n",
    "    if args.dataset_type == 'llff':\n",
    "        images, poses, bds, render_poses, i_test = load_llff_data(args.datadir, args.factor,\n",
    "                                                                  recenter=True, bd_factor=.75,\n",
    "                                                                  spherify=args.spherify)\n",
    "        hwf = poses[0, :3, -1]\n",
    "        poses = poses[:, :3, :4]\n",
    "        print('Loaded llff', images.shape,\n",
    "              render_poses.shape, hwf, args.datadir)\n",
    "        if not isinstance(i_test, list):\n",
    "            i_test = [i_test]\n",
    "\n",
    "        if args.llffhold > 0:\n",
    "            print('Auto LLFF holdout,', args.llffhold)\n",
    "            i_test = np.arange(images.shape[0])[::args.llffhold]\n",
    "\n",
    "        i_val = i_test\n",
    "        i_train = np.array([i for i in np.arange(int(images.shape[0])) if\n",
    "                            (i not in i_test and i not in i_val)])\n",
    "\n",
    "        print('DEFINING BOUNDS')\n",
    "        if args.no_ndc:\n",
    "            near = tf.reduce_min(bds) * .9\n",
    "            far = tf.reduce_max(bds) * 1.\n",
    "        else:\n",
    "            near = 0.\n",
    "            far = 1.\n",
    "        print('NEAR FAR', near, far)\n",
    "\n",
    "    elif args.dataset_type == 'blender':\n",
    "        images, poses, render_poses, hwf, i_split = load_blender_data(\n",
    "            args.datadir, args.half_res, args.testskip)\n",
    "        print('Loaded blender', images.shape,\n",
    "              render_poses.shape, hwf, args.datadir)\n",
    "        i_train, i_val, i_test = i_split\n",
    "\n",
    "        near = 2.\n",
    "        far = 6.\n",
    "\n",
    "        if args.white_bkgd:\n",
    "            images = images[..., :3]*images[..., -1:] + (1.-images[..., -1:])\n",
    "        else:\n",
    "            images = images[..., :3]\n",
    "\n",
    "    elif args.dataset_type == 'deepvoxels':\n",
    "\n",
    "        images, poses, render_poses, hwf, i_split = load_dv_data(scene=args.shape,\n",
    "                                                                 basedir=args.datadir,\n",
    "                                                                 testskip=args.testskip)\n",
    "\n",
    "        print('Loaded deepvoxels', images.shape,\n",
    "              render_poses.shape, hwf, args.datadir)\n",
    "        i_train, i_val, i_test = i_split\n",
    "\n",
    "        hemi_R = np.mean(np.linalg.norm(poses[:, :3, -1], axis=-1))\n",
    "        near = hemi_R-1.\n",
    "        far = hemi_R+1.\n",
    "\n",
    "    else:\n",
    "        print('Unknown dataset type', args.dataset_type, 'exiting')\n",
    "        return\n",
    "\n",
    "    # Cast intrinsics to right types\n",
    "    H, W, focal = hwf\n",
    "    H, W = int(H), int(W)\n",
    "    hwf = [H, W, focal]\n",
    "\n",
    "    if args.render_test:\n",
    "        render_poses = np.array(poses[i_test])\n",
    "\n",
    "    # Create log dir and copy the config file\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "    os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n",
    "    f = os.path.join(basedir, expname, 'args.txt')\n",
    "    with open(f, 'w') as file:\n",
    "        for arg in sorted(vars(args)):\n",
    "            attr = getattr(args, arg)\n",
    "            file.write('{} = {}\\n'.format(arg, attr))\n",
    "    if args.config is not None:\n",
    "        f = os.path.join(basedir, expname, 'config.txt')\n",
    "        with open(f, 'w') as file:\n",
    "            file.write(open(args.config, 'r').read())\n",
    "\n",
    "    # Create nerf model\n",
    "    render_kwargs_train, render_kwargs_test, start, grad_vars, models = create_nerf(\n",
    "        args)\n",
    "    rgbd_grad_vars, mu_grad_vars = grad_vars\n",
    "\n",
    "    bds_dict = {\n",
    "        'near': tf.cast(near, tf.float32),\n",
    "        'far': tf.cast(far, tf.float32),\n",
    "    }\n",
    "    render_kwargs_train.update(bds_dict)\n",
    "    render_kwargs_test.update(bds_dict)\n",
    "\n",
    "    # Short circuit if only rendering out from trained model\n",
    "    if args.render_only:\n",
    "        print('RENDER ONLY')\n",
    "        if args.render_test:\n",
    "            # render_test switches to test poses\n",
    "            images = images[i_test]\n",
    "        else:\n",
    "            # Default is smoother render_poses path\n",
    "            images = None\n",
    "\n",
    "        testsavedir = os.path.join(basedir, expname, 'renderonly_{}_{:06d}'.format(\n",
    "            'test' if args.render_test else 'path', start))\n",
    "        os.makedirs(testsavedir, exist_ok=True)\n",
    "        print('test poses shape', render_poses.shape)\n",
    "\n",
    "        rgbs, _ = render_path(render_poses, hwf, args.chunk, render_kwargs_test,\n",
    "                              gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor)\n",
    "        print('Done rendering', testsavedir)\n",
    "        imageio.mimwrite(os.path.join(testsavedir, 'video.mp4'),\n",
    "                         to8b(rgbs), fps=30, quality=8)\n",
    "\n",
    "        return\n",
    "\n",
    "    # Create optimizer\n",
    "    lrate = args.lrate\n",
    "    if args.lrate_decay > 0:\n",
    "        lrate = tf.keras.optimizers.schedules.ExponentialDecay(lrate,\n",
    "                                                               decay_steps=args.lrate_decay * 1000, decay_rate=0.1)\n",
    "    optimizer = tf.keras.optimizers.Adam(lrate)\n",
    "    models['optimizer'] = optimizer\n",
    "\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "    global_step.assign(start)\n",
    "\n",
    "    # Prepare raybatch tensor if batching random rays\n",
    "    N_rand = args.N_rand\n",
    "    use_batching = not args.no_batching\n",
    "    if use_batching:\n",
    "        # For random ray batching.\n",
    "        #\n",
    "        # Constructs an array 'rays_rgb' of shape [N*H*W, 3, 3] where axis=1 is\n",
    "        # interpreted as,\n",
    "        #   axis=0: ray origin in world space\n",
    "        #   axis=1: ray direction in world space\n",
    "        #   axis=2: observed RGB color of pixel\n",
    "        print('get rays')\n",
    "        # get_rays_np() returns rays_origin=[H, W, 3], rays_direction=[H, W, 3]\n",
    "        # for each pixel in the image. This stack() adds a new dimension.\n",
    "        rays = [get_rays_np(H, W, focal, p) for p in poses[:, :3, :4]]\n",
    "        rays = np.stack(rays, axis=0)  # [N, ro+rd, H, W, 3]\n",
    "        print('done, concats')\n",
    "        # [N, ro+rd+rgb, H, W, 3]\n",
    "        rays_rgb = np.concatenate([rays, images[:, None, ...]], 1)\n",
    "        # [N, H, W, ro+rd+rgb, 3]\n",
    "        rays_rgb = np.transpose(rays_rgb, [0, 2, 3, 1, 4])\n",
    "        rays_rgb = np.stack([rays_rgb[i]\n",
    "                             for i in i_train], axis=0)  # train images only\n",
    "        # [(N-1)*H*W, ro+rd+rgb, 3]\n",
    "        rays_rgb = np.reshape(rays_rgb, [-1, 3, 3])\n",
    "        rays_rgb = rays_rgb.astype(np.float32)\n",
    "        print('shuffle rays')\n",
    "        np.random.shuffle(rays_rgb)\n",
    "        print('done')\n",
    "        i_batch = 0\n",
    "\n",
    "    N_iters = 1000000\n",
    "    print('Begin')\n",
    "    print('TRAIN views are', i_train)\n",
    "    print('TEST views are', i_test)\n",
    "    print('VAL views are', i_val)\n",
    "\n",
    "    # Summary writers\n",
    "    writer = tf.contrib.summary.create_file_writer(\n",
    "        os.path.join(basedir, 'summaries', expname))\n",
    "    writer.set_as_default()\n",
    "\n",
    "    for i in range(start, N_iters):\n",
    "        time0 = time.time()\n",
    "\n",
    "        # Sample random ray batch\n",
    "\n",
    "        if use_batching:\n",
    "            # Random over all images\n",
    "            batch = rays_rgb[i_batch:i_batch+N_rand]  # [B, 2+1, 3*?]\n",
    "            batch = tf.transpose(batch, [1, 0, 2])\n",
    "\n",
    "            # batch_rays[i, n, xyz] = ray origin or direction, example_id, 3D position\n",
    "            # target_s[n, rgb] = example_id, observed color.\n",
    "            batch_rays, target_s = batch[:2], batch[2]\n",
    "\n",
    "            i_batch += N_rand\n",
    "            if i_batch >= rays_rgb.shape[0]:\n",
    "                np.random.shuffle(rays_rgb)\n",
    "                i_batch = 0\n",
    "\n",
    "        else:\n",
    "            # Random from one image\n",
    "            img_i = np.random.choice(i_train)\n",
    "            target = images[img_i]\n",
    "            pose = poses[img_i, :3, :4]\n",
    "\n",
    "            if N_rand is not None:\n",
    "                rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "                if i < args.precrop_iters:\n",
    "                    dH = int(H//2 * args.precrop_frac)\n",
    "                    dW = int(W//2 * args.precrop_frac)\n",
    "                    coords = tf.stack(tf.meshgrid(\n",
    "                        tf.range(H//2 - dH, H//2 + dH), \n",
    "                        tf.range(W//2 - dW, W//2 + dW), \n",
    "                        indexing='ij'), -1)\n",
    "                    if i < 10:\n",
    "                        print('precrop', dH, dW, coords[0,0], coords[-1,-1])\n",
    "                else:\n",
    "                    coords = tf.stack(tf.meshgrid(\n",
    "                        tf.range(H), tf.range(W), indexing='ij'), -1)\n",
    "                coords = tf.reshape(coords, [-1, 2])\n",
    "                select_inds = np.random.choice(\n",
    "                    coords.shape[0], size=[N_rand], replace=False)\n",
    "                select_inds = tf.gather_nd(coords, select_inds[:, tf.newaxis])\n",
    "                rays_o = tf.gather_nd(rays_o, select_inds)\n",
    "                rays_d = tf.gather_nd(rays_d, select_inds)\n",
    "                batch_rays = tf.stack([rays_o, rays_d], 0)\n",
    "                target_s = tf.gather_nd(target, select_inds)\n",
    "\n",
    "        #####  Core optimization loop  #####\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Make predictions for color, disparity, accumulated opacity.\n",
    "            rgb, disp, acc, mu, mu_exp, extras = render(\n",
    "                H, W, focal, chunk=args.chunk, rays=batch_rays,\n",
    "                verbose=i < 10, retraw=True, **render_kwargs_train)\n",
    "\n",
    "            # Compute MSE loss between predicted and true RGB.\n",
    "            img_loss = img2mse(rgb, target_s)\n",
    "            trans = extras['raw'][..., -1]\n",
    "            loss = img_loss\n",
    "            psnr = mse2psnr(img_loss)\n",
    "            mu_loss = tf.reduce_mean(tf.keras.losses.MSE(mu, mu_exp))\n",
    "\n",
    "            # Add MSE loss for coarse-grained model\n",
    "            if 'rgb0' in extras:\n",
    "                img_loss0 = img2mse(extras['rgb0'], target_s)\n",
    "                loss += img_loss0\n",
    "                psnr0 = mse2psnr(img_loss0)\n",
    "                mu0, mu_exp0 = extras['mu_map0'], extras['mu_exp0']\n",
    "                mu_loss0 = tf.reduce_mean(tf.keras.losses.MSE(mu0, mu_exp0))\n",
    "                mu_loss += mu_loss0\n",
    "        \n",
    "        gradients = tape.gradient(loss, rgbd_grad_vars)\n",
    "        optimizer.apply_gradients(zip(gradients, rgbd_grad_vars))\n",
    "        \n",
    "        # Start updating mu grad vars after 1000 iterations\n",
    "        if i > 1000:\n",
    "            mu_gradients = tape.gradient(mu_loss, mu_grad_vars)\n",
    "            optimizer.apply_gradients(zip(mu_gradients, mu_grad_vars))\n",
    "\n",
    "        dt = time.time()-time0\n",
    "\n",
    "        #####           end            #####\n",
    "\n",
    "        # Rest is logging\n",
    "\n",
    "        def save_weights(net, prefix, i):\n",
    "            path = os.path.join(\n",
    "                basedir, expname, '{}_{:06d}.npy'.format(prefix, i))\n",
    "            np.save(path, net.get_weights())\n",
    "            print('saved weights at', path)\n",
    "\n",
    "        if i % args.i_weights == 0:\n",
    "            for k in models:\n",
    "                save_weights(models[k], k, i)\n",
    "\n",
    "        if i % args.i_video == 0 and i > 0:\n",
    "\n",
    "            rgbs, disps = render_path(\n",
    "                render_poses, hwf, args.chunk, render_kwargs_test)\n",
    "            print('Done, saving', rgbs.shape, disps.shape)\n",
    "            moviebase = os.path.join(\n",
    "                basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n",
    "            imageio.mimwrite(moviebase + 'rgb.mp4',\n",
    "                             to8b(rgbs), fps=30, quality=8)\n",
    "            imageio.mimwrite(moviebase + 'disp.mp4',\n",
    "                             to8b(disps / np.max(disps)), fps=30, quality=8)\n",
    "\n",
    "            if args.use_viewdirs:\n",
    "                render_kwargs_test['c2w_staticcam'] = render_poses[0][:3, :4]\n",
    "                rgbs_still, _ = render_path(\n",
    "                    render_poses, hwf, args.chunk, render_kwargs_test)\n",
    "                render_kwargs_test['c2w_staticcam'] = None\n",
    "                imageio.mimwrite(moviebase + 'rgb_still.mp4',\n",
    "                                 to8b(rgbs_still), fps=30, quality=8)\n",
    "\n",
    "        if i % args.i_testset == 0 and i > 0:\n",
    "            testsavedir = os.path.join(\n",
    "                basedir, expname, 'testset_{:06d}'.format(i))\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            print('test poses shape', poses[i_test].shape)\n",
    "            render_path(poses[i_test], hwf, args.chunk, render_kwargs_test,\n",
    "                        gt_imgs=images[i_test], savedir=testsavedir)\n",
    "            print('Saved test set')\n",
    "\n",
    "        if i % args.i_print == 0 or i < 10:\n",
    "\n",
    "            print(expname, i, psnr.numpy(), loss.numpy(), global_step.numpy())\n",
    "            print('iter time {:.05f}'.format(dt))\n",
    "            with tf.contrib.summary.record_summaries_every_n_global_steps(args.i_print):\n",
    "                tf.contrib.summary.scalar('loss', loss)\n",
    "                tf.contrib.summary.scalar('psnr', psnr)\n",
    "                tf.contrib.summary.histogram('tran', trans)\n",
    "                if args.N_importance > 0:\n",
    "                    tf.contrib.summary.scalar('psnr0', psnr0)\n",
    "\n",
    "            if i % args.i_img == 0:\n",
    "\n",
    "                # Log a rendered validation view to Tensorboard\n",
    "                img_i = np.random.choice(i_val)\n",
    "                target = images[img_i]\n",
    "                pose = poses[img_i, :3, :4]\n",
    "\n",
    "                rgb, disp, acc, extras = render(H, W, focal, chunk=args.chunk, c2w=pose,\n",
    "                                                **render_kwargs_test)\n",
    "\n",
    "                psnr = mse2psnr(img2mse(rgb, target))\n",
    "                \n",
    "                # Save out the validation image for Tensorboard-free monitoring\n",
    "                testimgdir = os.path.join(basedir, expname, 'tboard_val_imgs')\n",
    "                if i==0:\n",
    "                    os.makedirs(testimgdir, exist_ok=True)\n",
    "                imageio.imwrite(os.path.join(testimgdir, '{:06d}.png'.format(i)), to8b(rgb))\n",
    "\n",
    "                with tf.contrib.summary.record_summaries_every_n_global_steps(args.i_img):\n",
    "\n",
    "                    tf.contrib.summary.image('rgb', to8b(rgb)[tf.newaxis])\n",
    "                    tf.contrib.summary.image(\n",
    "                        'disp', disp[tf.newaxis, ..., tf.newaxis])\n",
    "                    tf.contrib.summary.image(\n",
    "                        'acc', acc[tf.newaxis, ..., tf.newaxis])\n",
    "\n",
    "                    tf.contrib.summary.scalar('psnr_holdout', psnr)\n",
    "                    tf.contrib.summary.image('rgb_holdout', target[tf.newaxis])\n",
    "\n",
    "                if args.N_importance > 0:\n",
    "\n",
    "                    with tf.contrib.summary.record_summaries_every_n_global_steps(args.i_img):\n",
    "                        tf.contrib.summary.image(\n",
    "                            'rgb0', to8b(extras['rgb0'])[tf.newaxis])\n",
    "                        tf.contrib.summary.image(\n",
    "                            'disp0', extras['disp0'][tf.newaxis, ..., tf.newaxis])\n",
    "                        tf.contrib.summary.image(\n",
    "                            'z_std', extras['z_std'][tf.newaxis, ..., tf.newaxis])\n",
    "\n",
    "        global_step.assign_add(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "usage: ipykernel_launcher [-h] [--config CONFIG] [--expname EXPNAME]\n                          [--basedir BASEDIR] [--datadir DATADIR]\n                          [--netdepth NETDEPTH] [--netwidth NETWIDTH]\n                          [--netdepth_fine NETDEPTH_FINE]\n                          [--netwidth_fine NETWIDTH_FINE] [--N_rand N_RAND]\n                          [--lrate LRATE] [--lrate_decay LRATE_DECAY]\n                          [--chunk CHUNK] [--netchunk NETCHUNK]\n                          [--no_batching] [--no_reload] [--ft_path FT_PATH]\n                          [--random_seed RANDOM_SEED]\n                          [--precrop_iters PRECROP_ITERS]\n                          [--precrop_frac PRECROP_FRAC]\n                          [--N_samples N_SAMPLES]\n                          [--N_importance N_IMPORTANCE] [--perturb PERTURB]\n                          [--use_viewdirs] [--i_embed I_EMBED]\n                          [--multires MULTIRES]\n                          [--multires_views MULTIRES_VIEWS]\n                          [--raw_noise_std RAW_NOISE_STD] [--render_only]\n                          [--render_test] [--render_factor RENDER_FACTOR]\n                          [--dataset_type DATASET_TYPE] [--testskip TESTSKIP]\n                          [--shape SHAPE] [--white_bkgd] [--half_res]\n                          [--factor FACTOR] [--no_ndc] [--lindisp]\n                          [--spherify] [--llffhold LLFFHOLD]\n                          [--i_print I_PRINT] [--i_img I_IMG]\n                          [--i_weights I_WEIGHTS] [--i_testset I_TESTSET]\n                          [--i_video I_VIDEO]\nipykernel_launcher: error: ambiguous option: --f=C:\\Users\\tlabs\\AppData\\Local\\Temp\\tmp-16980570KuS1SMODQ.json could match --ft_path, --factor\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "2",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}