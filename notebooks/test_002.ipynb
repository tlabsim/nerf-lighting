{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0dffb5442d84be890217d664a91c46e420962c190ec91d07047007dfcc46efe8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.01831564>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "tf.exp(-tf.nn.relu(4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.006692851>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(x, y): return tf.reduce_mean(tf.square(x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.16624723889986415"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "mu = np.random.random((2048, 64))\n",
    "mu_exp = np.random.random((2048, 64))\n",
    "loss = tf.reduce_mean(tf.keras.losses.MSE(mu,  mu_exp))\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.16624723889986412"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "mse(mu_exp, mu).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = tf.convert_to_tensor(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9999944945769437"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "np.max(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((5, 4, 3), dtype = np.float32)\n",
    "l = np.array([2., 1., -3.], dtype = np.float32)\n",
    "a - l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_illumination(cam, light_data):\n",
    "    ambient = light_data['ambient']\n",
    "    if ambient < 0.0 : ambient = 0.0\n",
    "    if ambient > 1.0 : ambient = 1.0\n",
    "\n",
    "    # calculate contribution of each light source\n",
    "    for l in light_data:\n",
    "        loc = l['loc']\n",
    "        intensity = l['intensity']\n",
    "        view_dirs = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def raw2outputs(raw, z_vals, rays_d):        \n",
    "        def raw2alpha(raw, dists, act_fn=tf.nn.relu): return 1.0 - \\\n",
    "            tf.exp(-act_fn(raw) * dists)\n",
    "       \n",
    "        dists = z_vals[..., 1:] - z_vals[..., :-1]     \n",
    "        dists = tf.concat(\n",
    "            [dists, tf.broadcast_to([1e10], dists[..., :1].shape)],\n",
    "            axis=-1)  # [N_rays, N_samples]        \n",
    "        dists = dists * tf.linalg.norm(rays_d[..., None, :], axis=-1)\n",
    "        rgb = tf.math.sigmoid(raw[..., :3])  # [N_rays, N_samples, 3]\n",
    "        noise = 0.\n",
    "        if raw_noise_std > 0.:\n",
    "            noise = tf.random.normal(raw[..., 3].shape) * raw_noise_std\n",
    "\n",
    "        alpha = raw2alpha(raw[..., 3] + noise, dists)  # [N_rays, N_samples]\n",
    "        mu_expected = tf.math.cumprod(1.-alpha + 1e-10, axis=-1, exclusive=True)\n",
    "        weights = alpha * mu_expected                 \n",
    "        rgb_map = tf.reduce_sum(weights[..., None] * rgb, axis=-2)  # [N_rays, 3]\n",
    "        depth_map = tf.reduce_sum(weights * z_vals, axis=-1)\n",
    "        ws = tf.maximum(1e-10, tf.reduce_sum(weights, axis=-1))\n",
    "        disp_map = 1./tf.maximum(1e-10, depth_map / ws)\n",
    "        acc_map = tf.reduce_sum(weights, -1)\n",
    "        mu_map = tf.nn.relu(raw[..., 4])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_diffuse(ray_batch,\n",
    "                network_fn,\n",
    "                network_query_fn,\n",
    "                N_samples,\n",
    "                retraw=False,\n",
    "                lindisp=False,\n",
    "                perturb=0.,\n",
    "                N_importance=0,\n",
    "                network_fine=None,\n",
    "                white_bkgd=False,\n",
    "                raw_noise_std=0.,\n",
    "                lighting_data = None\n",
    "                verbose=False):\n",
    "    \"\"\"Volumetric rendering.\n",
    "\n",
    "    Args:\n",
    "      ray_batch: array of shape [batch_size, ...]. All information necessary\n",
    "        for sampling along a ray, including: ray origin, ray direction, min\n",
    "        dist, max dist, and unit-magnitude viewing direction.\n",
    "      network_fn: function. Model for predicting RGB and density at each point\n",
    "        in space.\n",
    "      network_query_fn: function used for passing queries to network_fn.\n",
    "      N_samples: int. Number of different times to sample along each ray.\n",
    "      retraw: bool. If True, include model's raw, unprocessed predictions.\n",
    "      lindisp: bool. If True, sample linearly in inverse depth rather than in depth.\n",
    "      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified\n",
    "        random points in time.\n",
    "      N_importance: int. Number of additional times to sample along each ray.\n",
    "        These samples are only passed to network_fine.\n",
    "      network_fine: \"fine\" network with same spec as network_fn.\n",
    "      white_bkgd: bool. If True, assume a white background.\n",
    "      raw_noise_std: ...\n",
    "      verbose: bool. If True, print more debugging info.\n",
    "\n",
    "    Returns:\n",
    "      rgb_map: [num_rays, 3]. Estimated RGB color of a ray. Comes from fine model.\n",
    "      disp_map: [num_rays]. Disparity map. 1 / depth.\n",
    "      acc_map: [num_rays]. Accumulated opacity along each ray. Comes from fine model.\n",
    "      raw: [num_rays, num_samples, 4]. Raw predictions from model.\n",
    "      rgb0: See rgb_map. Output for coarse model.\n",
    "      disp0: See disp_map. Output for coarse model.\n",
    "      acc0: See acc_map. Output for coarse model.\n",
    "      z_std: [num_rays]. Standard deviation of distances along ray for each\n",
    "        sample.\n",
    "    \"\"\"\n",
    "\n",
    "    def raw2outputs(raw, z_vals, rays_d):\n",
    "        \"\"\"Transforms model's predictions to semantically meaningful values.\n",
    "\n",
    "        Args:\n",
    "          raw: [num_rays, num_samples along ray, 4]. Prediction from model.\n",
    "          z_vals: [num_rays, num_samples along ray]. Integration time.\n",
    "          rays_d: [num_rays, 3]. Direction of each ray.\n",
    "\n",
    "        Returns:\n",
    "          rgb_map: [num_rays, 3]. Estimated RGB color of a ray.\n",
    "          disp_map: [num_rays]. Disparity map. Inverse of depth map.\n",
    "          acc_map: [num_rays]. Sum of weights along each ray.\n",
    "          weights: [num_rays, num_samples]. Weights assigned to each sampled color.\n",
    "          depth_map: [num_rays]. Estimated distance to object.\n",
    "        \"\"\"\n",
    "        # Function for computing density from model prediction. This value is\n",
    "        # strictly between [0, 1].\n",
    "        def raw2alpha(raw, dists, act_fn=tf.nn.relu): return 1.0 - \\\n",
    "            tf.exp(-act_fn(raw) * dists)\n",
    "\n",
    "        # Compute 'distance' (in time) between each integration time along a ray.\n",
    "        dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "\n",
    "        # The 'distance' from the last integration time is infinity.\n",
    "        dists = tf.concat(\n",
    "            [dists, tf.broadcast_to([1e10], dists[..., :1].shape)],\n",
    "            axis=-1)  # [N_rays, N_samples]\n",
    "\n",
    "        # Multiply each distance by the norm of its corresponding direction ray\n",
    "        # to convert to real world distance (accounts for non-unit directions).\n",
    "        dists = dists * tf.linalg.norm(rays_d[..., None, :], axis=-1)\n",
    "\n",
    "        # Extract RGB of each sample position along each ray.\n",
    "        rgb = tf.math.sigmoid(raw[..., :3])  # [N_rays, N_samples, 3]\n",
    "\n",
    "        # Add noise to model's predictions for density. Can be used to \n",
    "        # regularize network during training (prevents floater artifacts).\n",
    "        noise = 0.\n",
    "        if raw_noise_std > 0.:\n",
    "            noise = tf.random.normal(raw[..., 3].shape) * raw_noise_std\n",
    "\n",
    "        # Predict density of each sample along each ray. Higher values imply\n",
    "        # higher likelihood of being absorbed at this point.\n",
    "        alpha = raw2alpha(raw[..., 3] + noise, dists)  # [N_rays, N_samples]\n",
    "\n",
    "        # Compute weight for RGB of each sample along each ray.  A cumprod() is\n",
    "        # used to express the idea of the ray not having reflected up to this\n",
    "        # sample yet.\n",
    "        # [N_rays, N_samples]\n",
    "\n",
    "        mu_expected = tf.math.cumprod(1.-alpha + 1e-10, axis=-1, exclusive=True)\n",
    "        weights = alpha * mu_expected                 \n",
    "\n",
    "        # Computed weighted color of each sample along each ray.\n",
    "        rgb_map = tf.reduce_sum(weights[..., None] * rgb, axis=-2)  # [N_rays, 3]\n",
    "\n",
    "        # Estimated depth map is expected distance.\n",
    "        depth_map = tf.reduce_sum(weights * z_vals, axis=-1)\n",
    "        ws = tf.maximum(1e-10, tf.reduce_sum(weights, axis=-1))\n",
    "\n",
    "        # Disparity map is inverse depth.\n",
    "        disp_map = 1./tf.maximum(1e-10, depth_map / ws)\n",
    "\n",
    "        # Sum of weights along each ray. This value is in [0, 1] up to numerical error.\n",
    "        acc_map = tf.reduce_sum(weights, -1)\n",
    "\n",
    "        mu_map = tf.nn.relu(raw[..., 4])   \n",
    "\n",
    "        if lighting_data is not None:\n",
    "            ambient = lighting_data['ambient']\n",
    "            lights = lighting_data['lights']\n",
    "\n",
    "            original_rgb = rgb_map\n",
    "            \n",
    "            if ambient >= 0.0 and ambient <= 1.0\n",
    "            rgb_map = rgb_map * ambient\n",
    "            \n",
    "            for l in lights:\n",
    "                loc = l['loc']\n",
    "                intensity = l['intensity']\n",
    "                '''\n",
    "                TODO::\n",
    "                1. for each samples position\n",
    "                '''\n",
    "                \n",
    "\n",
    "\n",
    "        # To composite onto a white background, use the accumulated alpha map.\n",
    "        if white_bkgd:\n",
    "            rgb_map = rgb_map + (1.-acc_map[..., None])\n",
    "\n",
    "        return rgb_map, disp_map, acc_map, weights, depth_map, mu_map, mu_expected\n",
    "\n",
    "    ###############################\n",
    "    # batch size\n",
    "    N_rays = ray_batch.shape[0]\n",
    "\n",
    "    # Extract ray origin, direction.\n",
    "    rays_o, rays_d = ray_batch[:, 0:3], ray_batch[:, 3:6]  # [N_rays, 3] each\n",
    "\n",
    "    # Extract unit-normalized viewing direction.\n",
    "    viewdirs = ray_batch[:, -3:] if ray_batch.shape[-1] > 8 else None\n",
    "\n",
    "    # Extract lower, upper bound for ray distance.\n",
    "    bounds = tf.reshape(ray_batch[..., 6:8], [-1, 1, 2])\n",
    "    near, far = bounds[..., 0], bounds[..., 1]  # [-1,1]\n",
    "\n",
    "    # Decide where to sample along each ray. Under the logic, all rays will be sampled at\n",
    "    # the same times.\n",
    "    t_vals = tf.linspace(0., 1., N_samples)\n",
    "    if not lindisp:\n",
    "        # Space integration times linearly between 'near' and 'far'. Same\n",
    "        # integration points will be used for all rays.\n",
    "        z_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "    else:\n",
    "        # Sample linearly in inverse depth (disparity).\n",
    "        z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "    z_vals = tf.broadcast_to(z_vals, [N_rays, N_samples])\n",
    "\n",
    "    # Perturb sampling time along each ray.\n",
    "    if perturb > 0.:\n",
    "        # get intervals between samples\n",
    "        mids = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "        upper = tf.concat([mids, z_vals[..., -1:]], -1)\n",
    "        lower = tf.concat([z_vals[..., :1], mids], -1)\n",
    "        # stratified samples in those intervals\n",
    "        t_rand = tf.random.uniform(z_vals.shape)\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "    # Points in space to evaluate model at.\n",
    "    pts = rays_o[..., None, :] + rays_d[..., None, :] * \\\n",
    "        z_vals[..., :, None]  # [N_rays, N_samples, 3]\n",
    "\n",
    "    # Evaluate model at each point.\n",
    "    raw = network_query_fn(pts, viewdirs, network_fn)  # [N_rays, N_samples, 4]\n",
    "    rgb_map, disp_map, acc_map, weights, depth_map, mu_map, mu_exp = \\\n",
    "        raw2outputs(raw, z_vals, rays_d)\n",
    "\n",
    "    if N_importance > 0:\n",
    "        rgb_map_0, disp_map_0, acc_map_0, mu_map_0, mu_exp_0 = \\\n",
    "            rgb_map, disp_map, acc_map, mu_map, mu_exp\n",
    "\n",
    "        # Obtain additional integration times to evaluate based on the weights\n",
    "        # assigned to colors in the coarse model.\n",
    "        z_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "        z_samples = sample_pdf(\n",
    "            z_vals_mid, weights[..., 1:-1], N_importance, det=(perturb == 0.))\n",
    "        z_samples = tf.stop_gradient(z_samples)\n",
    "\n",
    "        # Obtain all points to evaluate color, density at.\n",
    "        z_vals = tf.sort(tf.concat([z_vals, z_samples], -1), -1)\n",
    "        pts = rays_o[..., None, :] + rays_d[..., None, :] * \\\n",
    "            z_vals[..., :, None]  # [N_rays, N_samples + N_importance, 3]\n",
    "\n",
    "        # Make predictions with network_fine.\n",
    "        run_fn = network_fn if network_fine is None else network_fine\n",
    "        raw = network_query_fn(pts, viewdirs, run_fn)\n",
    "        rgb_map, disp_map, acc_map, weights, depth_map, mu_map, mu_exp = \\\n",
    "            raw2outputs(raw, z_vals, rays_d)\n",
    "\n",
    "    ret = {'rgb_map': rgb_map, 'disp_map': disp_map, 'acc_map': acc_map, \\\n",
    "        'mu_map': mu_map, 'mu_exp': mu_exp}\n",
    "    if retraw:\n",
    "        ret['raw'] = raw\n",
    "    if N_importance > 0:\n",
    "        ret['rgb0'] = rgb_map_0\n",
    "        ret['disp0'] = disp_map_0\n",
    "        ret['acc0'] = acc_map_0        \n",
    "        ret['mu_map0'] = mu_map_0\n",
    "        ret['mu_exp0'] = mu_exp_0\n",
    "        ret['z_std'] = tf.math.reduce_std(z_samples, -1)  # [N_rays]\n",
    "\n",
    "    for k in ret:\n",
    "        tf.debugging.check_numerics(ret[k], 'output {}'.format(k))\n",
    "\n",
    "    return ret\n"
   ]
  }
 ]
}